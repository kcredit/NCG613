{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c5fd1a",
   "metadata": {},
   "source": [
    "# Regression Models for Spatial Data\n",
    "**NCG613: Data Analytics Project - Practical 7**\n",
    "\n",
    "This notebook investigates the use of regression methods using ```spreg``` to analyse housing prices in a hedonic framework. I have prepared a range of data for this practical (in order to save significant amounts of computation time), including a few demographic variables at the Output Area (small area) Census geography and several spatial features involving distance (in metres) from housing points to various urban features of interest, including __primary roads__, __transit stations__, and __open space__. In general, the [London Datastore](https://data.london.gov.uk/) is a good place to find spatial data on various urban features, and UK Census data can be accessed easily through [NOMIS](https://www.nomisweb.co.uk/). \n",
    "\n",
    "Specifically, data used to prepare variables in this practical come from the following sources:\n",
    "\n",
    "- Distance to nearest open space: [Greenspace Information for Greater London](https://data.london.gov.uk/dataset/access-public-open-space-and-nature-ward)\n",
    "- Distance to nearest primary road: [London Atmospheric Emissions Inventory](https://data.london.gov.uk/dataset/london-atmospheric-emissions-inventory--laei--2016)\n",
    "- Distance to \"Kilometre Zero\" in London (i.e., centre of the city) near Trafalgar Square: [OpenStreetMap](https://wiki.openstreetmap.org/wiki/OSMPythonTools)\n",
    "- Distance to nearest transit station: [OpenStreetMap](https://wiki.openstreetmap.org/wiki/OSMPythonTools) features (`public_transit >> station`)\n",
    "\n",
    "*Note: Setup and installation instructions are in the repository README.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scipy\n",
    "# !conda install statsmodels\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23025d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (15, 10) #this increases the inline figure size to 15 tall x 10 wide\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pysal\n",
    "import numpy as np\n",
    "import mapclassify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Change settings so that warnings are not displayed\n",
    "\n",
    "import contextily as cx\n",
    "from shapely.geometry import Polygon\n",
    "import plotly.express as px\n",
    "from pysal.explore import esda\n",
    "from pysal.lib import weights\n",
    "from splot.esda import plot_moran\n",
    "from splot.esda import moran_scatterplot\n",
    "from splot.esda import plot_local_autocorrelation\n",
    "from splot.esda import lisa_cluster\n",
    "from esda.moran import Moran_Local\n",
    "\n",
    "# Loading a few new packages\n",
    "from scipy import stats\n",
    "from pysal.model import spreg\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()  # Enable progress_apply for progress bar visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beda570",
   "metadata": {},
   "source": [
    "Load in the required files: `road` (primary roads), `opens` (open space), `transit` (public transport stations), `oa` (Output Areas), `KM0` ('Kilometre zero', the centre of the city), and `lb` (the London Boroughs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91cee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "road = gpd.read_file('../data/Primary_Roads.geojson')\n",
    "opens = gpd.read_file('../data/OpenSpace.geojson')\n",
    "transit = gpd.read_file('../data/TransitStations.geojson')\n",
    "oa = gpd.read_file('../data/OutputAreas.geojson')\n",
    "KM0 = gpd.read_file('../data/KM_Zero.geojson')\n",
    "lb = gpd.read_file('../data/london_boroughs.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d010cac",
   "metadata": {},
   "source": [
    "Now we will engineer a few spatial features that are important predictors of house prices. We'll start by loading in the 2023 price point data, and then calculating the distance from each house point to the nearest primary road.\n",
    "\n",
    "*__Note:__ this process can take a long time, so I have pre-calculated these variables for the 2023 housing points (`hp`). If you'd like to skip these calculation steps, just run the cell below and then jump down to the next section ('Non-spatial Ordinary Least Squares (OLS) regression')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = gpd.read_file('../data/hp2023.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp = pd.read_csv('../data/hpdemo.csv')\n",
    "# hp2 = gpd.GeoDataFrame(\n",
    "#     hp, geometry=gpd.points_from_xy(x=hp.xcoord, y=hp.ycoord)\n",
    "# )\n",
    "# hp2.crs = \"epsg:27700\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpr = hp2.geometry.progress_apply(lambda x: road.distance(x).min())\n",
    "# hp3 = pd.merge(hp2, hpr, left_index=True, right_index=True)\n",
    "# data = hp3.rename(index=str, columns={\"geometry_y\":\"Dist_Road\", \"geometry_x\":\"geometry\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2198e",
   "metadata": {},
   "source": [
    "Next, we'll use a similar process to calculate distances to the nearest open space and transit station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271dcafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transit = data.geometry.progress_apply(lambda x: transit.distance(x).min())\n",
    "# data_transit_2 = pd.merge(data, data_transit, left_index=True, right_index=True)\n",
    "# data_transit_3 = data_transit_2.rename(index=str, columns={\"geometry_y\":\"Dist_Transit\", \"geometry_x\":\"geometry\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_opens = data_transit_3.geometry.progress_apply(lambda x: opens.distance(x).min())\n",
    "# data_opens_2 = pd.merge(data_transit_3, data_opens, left_index=True, right_index=True)\n",
    "# hp = data_opens_2.rename(index=str, columns={\"geometry_y\":\"Dist_OpenSpace\", \"geometry_x\":\"geometry\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54485ed",
   "metadata": {},
   "source": [
    "Finally, we'll add in the rough approximation of floor area that we used in practical 6, and calculate a price per floor area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp['fl_area'] = np.where(hp['type']== 'T', ((81+86)/2), \n",
    "#                           np.where(hp['type']== 'D', 152,\n",
    "#                                   np.where(hp['type']== 'S', 93,\n",
    "#                                            np.where(hp['type']== 'F', ((65+55)/2), \n",
    "#                                                     ((86+81+93+152+77+65+55)/7)))))\n",
    "# hp['priceper'] = hp.price/hp.fl_area\n",
    "# hp.to_file(\"hp2023.geojson\", driver='GeoJSON') # Export the calculated house price data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1973620b",
   "metadata": {},
   "source": [
    "We should now have all of the variables we need to start doing some hedonic regression analysis of the spatial factors that influence housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b609a4",
   "metadata": {},
   "source": [
    "## Non-spatial Ordinary Least Squares (OLS) regression\n",
    "Before we dive into regression specifics, let's take a look at the big picture: why do we use regression models to answer social science questions in the first place? A multiple regression is an extension of classic equation of a line $y = mx + b$ for more than one independent variable in which the best-fit line is calculated for the dependent variable (predicted values) based on the associated values of all the covariates (independent variables). The OLS function solves this equation (in multi-dimensional space) by minimizing the squared distance between the predicted and observed values of $Y$ (i.e., 'residuals'). Because the linear model is additive, we can interpret each individual slope coefficient $\\beta_{k}$ on the best-fit line as the (partial) correlation between the explanatory variable $k$ and the dependent variable $Y$, *holding all other explanatory variables constant*. From [Rey et al. (2020)](https://geographicdata.science/book/notebooks/11_regression.html#spatial-feature-engineering):\n",
    ">When one calculates bivariate correlations, the coefficient of a variable is picking up the correlation between the variables, but it is also subsuming into it variation associated with other correlated variables â€“ also called confounding factors. Regression allows us to isolate the distinct effect that a single variable has on the dependent one, once we control for those other variables\n",
    "\n",
    "This feature - combined with the fact that we can test the significance of each coefficient using a $p$-value (with the assumptions that the residuals of the model are normally-distributed and have constant variance) - makes the linear regression an incredibly powerful tool for assessing explanatory relationships between variables in the social (and other) sciences. While it is now incredibly easy to calculate linear regressions on the fly in many statistical software packages, it is valuable to appreciate the theoretical mathematical work that has gone in to creating this useful statistical tool. However, it is also important to understand that the linear model has limitations and is only unbiased within a set of somewhat-limited assumptions, which we'll explore here.\n",
    "\n",
    "### Hedonic Modelling\n",
    "This practical demonstrates the explanatory regression framework by using the example of a hedonic regression model. Hedonic models are used to explain the effect of various characteristics on urban housing values. With the unit of analysis at the house (point) level, the basic specification of the Hedonic model is: \n",
    "\n",
    "$Y = X_{1}\\beta_{1} + X_{2}\\beta_{2} + \\upsilon$\n",
    "\n",
    "where $Y$ is the (log-transformed) price, $X_{1}$ is a vector of housing characteristics (e.g., size, age, number of rooms, etc.),  $X_{2}$ is a vector of neighbourhood and urban characteristics (e.g., distance to the city centre, distance to the nearest transit station, etc.), and $\\upsilon$ is the error term (or constant). The first term of this equation is pretty straightforward - the basic assumption is that the price of a house is going to be significantly related to how large it is, and generally what kind of house it is. \n",
    "\n",
    "We can test this assumption by running a non-spatial regression between price and various housing characteristics. First, let's pull up the table so that we can see what the variables are called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c163ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9fca2",
   "metadata": {},
   "source": [
    "The first thing we need to do is to check the distribution of our continuous variables, in particular 'price' and 'fl_area' (while normal distribution for individual *variables* is not a core assumption of linear regression, transforming skewed variables by taking their natural log can provide a better linear fit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6611d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bx = seaborn.violinplot(x=hp[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac960960",
   "metadata": {},
   "outputs": [],
   "source": [
    "bx = seaborn.violinplot(x=(hp[\"fl_area\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9eebc9",
   "metadata": {},
   "source": [
    "Obviously the violin plot for 'fl_area' shows how it was interpolated purely as a function of dwelling type. However, both of these variables are fairly right-skewed, so we'll take the natural log of both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a43a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp['log_area'] = np.log(hp['fl_area'])\n",
    "hp['log_price'] = np.log(hp['price'])\n",
    "bx = seaborn.violinplot(x=hp[\"log_price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81507ffc",
   "metadata": {},
   "source": [
    "We'll also divide the three distance variables by 1,000 to transform the measurement into units of kilometres (rather than metres) which will help our interpretation of the regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp['Dist_Road'] = hp.Dist_Road/1000\n",
    "hp['Dist_Transit'] = hp.Dist_Transit/1000\n",
    "hp['Dist_OpenSpace'] = hp.Dist_OpenSpace/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a7e03",
   "metadata": {},
   "source": [
    "Now, as you can see, the distribution of 'log_price' is much more conducive to a linear fit with other generally-normally-distributed variables. To run a regression in ```spreg``` - which is a spatial regression package, but can also perform non-spatial regressions - we have to feed the function a vector of $Y$ and $X$ variables, so first we have to build a list of covariates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = ['log_area', 'Dist_Road', 'Dist_Transit', 'Dist_OpenSpace']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddc553",
   "metadata": {},
   "source": [
    "Then we call the model and print the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = spreg.OLS(hp[['log_price']].values, hp[variable_names].values,\n",
    "                name_y='log_price', name_x=variable_names, robust='white')\n",
    "print(m1.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3303f",
   "metadata": {},
   "source": [
    "The standard regression output in ```spreg``` gives us a lot of information, so let's take a minute to examine it. \n",
    "### Summary of Output: Top Section\n",
    "On the top we have some indicators of model fit, including the R-squared, which tells us the % of overall variance in $Y$ explained by the covariates that we included in this model. 0.1605 is about what we would expect given that we do not have very much information on the characteristics of each *house* - only a rough estimate of the floor area (based on dwelling type). Given that, it is fairly impressive that we have explained about a sixth of the total variance in housing prices just by including information on the size of the house and distance to a few urban amenities. Adjusted R-squared discounts the raw R-squared by taking into account the number of variables (because models with more covariates tend to have higher R-squareds simply as a by-product of having more information included to make a prediction), and is generally the one to use when evaluating models. We can also see the number of observations ($N$) and degrees of freedom (values that are free to vary in the calculation of the model, i.e., if you have as many independent variables as observations, your model estimates will be biased).\n",
    "### Summary of Output: Middle Section\n",
    "Next we can look at the coefficient values, which are the slopes of the regression line that the model calculated for each of the covariates that we included. As a note: the [UCLA Institute for Digital Research and Education (IDRE)](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/) has a lot of great guides for understanding basic parameters of statistical models, so feel free to read through (or search for) articles related to understanding the basics of regression models. Since our dependent variable is logged, the coefficients of this model can be a little difficult to interpret. For a typical continuous variable in any regression model - like 'Dist_Transit' - the coefficient can be interpreted as the change (positive or negative) in the *dependent variable* expected based on a one-unit change in the independent variable. However, since our dependent variable is logged, the interpretation changes a bit: first we have to exponentiate the coefficient (i.e., take the inverse of the natural log); this value (centred on 1) is the *percent change in the geometric mean* of the dependent variable that we would expect form a one-unit increase in the independent variable. Subtracting 1 from the exponentiated coefficient helps our interpretation. Let's look at the 'Dist_Transit' example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-0.1008359) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f41f8b",
   "metadata": {},
   "source": [
    "So based on our model, we can expect a 9.6% *decrease* in housing price for every km away from a public transport station it is located, *holding all other characteristics constant* (because these are partial correlations). This is pretty useful information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e8822",
   "metadata": {},
   "source": [
    "The final piece is how to interpret the coefficient on an independent variable in a model where the dependent variable is also logged (called an \"elasticity\"). Because these are both expressed in terms of % change, we have to specify the % increase in the independent variable that we are interested in observing change - let's say we're interested in what happens to housing price with a 10% increase in house size ('fl_area'). We take that 10% expressed as 1.10 and raise it to the coefficient value to find the value (centred on 1) of the percent change in the geometric mean of the dependent variable that we would expect from a 10% increase in the independent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.1)**1.0194316 - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db6421",
   "metadata": {},
   "source": [
    "So, based on this model, we expect to see a 10.2% increase in housing price for every 10% increase in housing size. \n",
    "\n",
    "The last piece of information that we should extract from the middle section of the report are the \"t-Statistic\" and \"probability\" columns. The [t-value](https://en.wikipedia.org/wiki/T-statistic) (used, as opposed to a $z$-score, when we don't know the population mean or standard deviation, as is the case in most empirical regression analysis) is a measure of the ratio of [\"signal\" to \"noise\"](https://blog.minitab.com/en/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen), i.e., the difference between the estimated regression coefficient and 0 (being the null hypothesis in a situation where we don't know the \"true\" coefficient) - the \"signal\" - divided by the standard error of the coefficient (essentially a measure of the coefficient's variation) - the \"noise\". We can find this simply by dividing the estimated coefficient (because the null hypothesis is 0) by the standard error for, e.g., 'log_area':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0194316/0.0082353"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aeb345",
   "metadata": {},
   "source": [
    "The idea is to provide a number associated with how confident we are that this coefficient value is different than 0. If the coefficient is very large but the error around the estimate is also large (i.e., we have a large estimate but it varies a lot from case to case), then we will have low confidence that the predicted coefficient is different than 0 (thus a low $t$-value). In that way, the significance of your coefficient estimate is a function of 1) actual effect of the independent variable, 2) variation in the relationship of the independent variable to the dependent variable, and 3) the sample size. A large $t$-value corresponds to a small probability ($p$-value) that the coefficient is equal to 0 (the null hypothesis); basically, the $p$-value is a comparison the $t$-value to the [*Student's t distribution*](https://www.itl.nist.gov/div898/handbook/eda/section3/eda3672.htm) to determine what % of values are \"more extreme\" than the estimated $t$-value. This varies by degrees of freedom in the model - with three degrees of freedom, a $t$-value of 10.215 is more extreme that 0.999% of values in the distribution (and thus would have a corresponding $p$-value of .001); but as degrees of freedom (i.e., sample size) increase, $t$-value threshold for given critical values also decreases. With large samples, $t$-values of 2.326 are more extreme than 0.99% of values in the Student's $t$ distribution.\n",
    "\n",
    "OK, so what does it all mean? Generally you want to set a critical $p$-value for rejecting/not rejecting the null hypothesis before you run your regression - the most common value is 0.05, but there are good reasons to select higher or lower values, depending on your data (critically, on your sample size). The $p$-value, then, is an estimate of the probability that the model's regression coefficient is not actually different than 0. If it is below the set threshold, we talk about the coefficient being a \"statistically-significant\" predictor of $Y$, but there are again good reasons to take $p$-values with a grain of salt and use more nuanced terms such as \"interesting\" values, or to discuss \"high confidence\" that the result is significant, etc. In our case - in this specification, at least - every variable seems to be highly-significant for predicting variation in housing price.\n",
    "\n",
    "### Summary of Output: Bottom Section\n",
    "Finally, at the bottom of the report we have the results of several diagnostic tests. The ```MULTICOLLINEARITY CONDITION NUMBER``` is a test of the collinearity between covariates - generally 51.846 is quite a high value, but this is also to be expected based on the framework of the hedonic model (distance to transit will tend to be highly-correlated with distance to roads, for instance). While [multicollinearity](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/) in the predictors can bias the coefficient estimates, cause swings in positive/negative signs, and create significant difficulties in interpretation (if two variables are almost entirely correlated, what does it mean for one coefficient to represent variation in $Y$ *holding constant* almost all of its own variation that is embodied in the other collinear variable?), the problem also has to be weighed against ommitted variable bias, which occurs when important predictors of $Y$ are left out of the model entirely. In our case, given the well-established nature of the hedonic model, we will sit tight with our multicollinearity condition value and observe if it swings up wildly from here with additional covariates. The ```Jarque-Bera``` statistics provides a test on the normality of errors of our model - in this case it is significant, meaning that the errors are significantly *not* normally-distributed. Since we have already logged a few of our variables to provide a more linear fit, the next step is to include additional factors - including spatial variables - that might improve the normality of the model errors. And finally, the ```Breusch-Pagan``` and ```Koenker-Bassett``` statistics test for heteroskedasticity. Again, in this case, both are significant, which indicates that the use of the ```robust='white'``` parameter in the ```spreg``` call to use White's robust standard errors is justified. While these values remain significant, unfortunately ```spreg``` does not include additional options for standard error correction, and we will have to be content with watching the value of these test statistics themselves increase or decrease with future specifications.\n",
    "\n",
    "## Spatial Structure\n",
    "One way to interrogate these model diagnostics - and particularly the non-normality of the errors - further is to try to understand if spatial structure is playing a hidden role in determining variation in $Y$. In our current specification, we haven't included any information about neighbourhood or any of the contextual urban factors that play a significant role in determining property values (a beautiful, new, gigantic mansion located next to an industrial site that pollutes the air will not be worth the same as it would be in a quiet residential neighbourhood).\n",
    "\n",
    "One way to examine this (and to help explain the non-normality of the model errors) is to look at clustering in the model errors by neighbourhood. To do this, let's spatially join the 2023 housing points to the Output Areas (in which I have embedded the name of the borough that each Output Area is located within):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545f91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp['residual'] = m1.u\n",
    "hp2 = gpd.sjoin(hp,oa)\n",
    "hp2 = hp2.dropna(subset=['PROFSCI'])\n",
    "pd.set_option('display.max_columns', None)\n",
    "hp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277394c7",
   "metadata": {},
   "source": [
    "And then plot the distribution of residuals by borough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935136ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = hp2.groupby(\"NAME_2\").residual.median().to_frame('hood_residual')\n",
    "\n",
    "f = plt.figure(figsize=(15,3))\n",
    "ax = plt.gca()\n",
    "seaborn.boxplot(\n",
    "    x='NAME_2', \n",
    "    y='residual', \n",
    "    ax = ax,\n",
    "    data=hp2.merge(\n",
    "        medians, how='left', left_on='NAME_2', right_index=True\n",
    "    ).sort_values('hood_residual'), \n",
    "    palette='bwr',\n",
    ")\n",
    "# Rotate the X labels for legibility\n",
    "f.autofmt_xdate(rotation=-90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a315936",
   "metadata": {},
   "source": [
    "As you can see, there is significant variation in how the model is working by neighbourhood! Neighbourhoods on the left of the scale have consistently lower observed values of housing price than the model would predict based on the characteristics we fed the model (i.e., negative residuals), and neighbourhoods on the right have consistently higher values than the model would predict (i.e., positive residuals). Why might that be? It appears that a lot of the boroughs on the high and low end of the residual spectrum are close to one another in space. We can test this explicitly by creating a __distance-based spatial weights matrix__ and generating spatial lags of the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights.DistanceBand.from_dataframe(hp2, 800) # Weights based on features within 800 meters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807b764",
   "metadata": {},
   "source": [
    "And then correlating these spatially-lagged residuals with the residuals of the model in their origin locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_residual = weights.spatial_lag.lag_spatial(w, hp2.residual)\n",
    "ax = seaborn.regplot(\n",
    "    x=hp2.residual,\n",
    "    y=lag_residual,\n",
    "    line_kws=dict(color='orangered'),\n",
    "    ci=None,\n",
    ")\n",
    "ax.set_xlabel('Model Residuals - $u$')\n",
    "ax.set_ylabel('Spatial Lag of Model Residuals - $W u$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76701af",
   "metadata": {},
   "source": [
    "This is called a __Moran's $I$__ plot of the model residuals: the slope of the regression line corresponds to the level of __spatial autocorrelation__ in the model residuals (because it shows the correlation between a given observation's residual and it's neighbours' residuals). We can see from the regression line here that there is a pretty large positive correlation between the residuals of the model in a given point and the residuals of the model in its neighbours. Thus we can see pretty clearly that the prediction errors are clustering! Not only is this a violation of the core OLS assumption, but it can also perhaps help us infer that there are spatial processes structuring the relationship of the variables we've chosen to housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a251eaa",
   "metadata": {},
   "source": [
    "And now we see that there is a really consistent geography to the pattern of over- and under-prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0eee11",
   "metadata": {},
   "source": [
    "### Spatial Heterogeneity and Fixed Effects\n",
    "Without a pre-conceived notion of what might be structuring these errors, one way to better control for the heterogenous relationships between the predictor variables and housing price by neighbourhood is just to insert dummy variables for each borough directly in the regression model as 'fixed effects'. These fixed effects then control for all of the unobserved variation (outside of the other covariates) associated with each borough, and can be a very useful way to observe heterogeneity in relationships across space. We'll also add in a few additional explanatory variables - 'DEPRHH' (% households with at least one deprivation indicator) and 'PROFSCI' (% employed in professional and scientific industries). To run this model, we'll use ```statsmodels``` to build the equation, because we're using non-spatial regressions and it has an easier interface in which to enter the equation directly. First we build the equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_variable_names = variable_names + ['DEPRHH'] + ['PROFSCI']\n",
    "f = 'log_price ~ ' + ' + '.join(added_variable_names) + ' + NAME_2 - 1' # Remove intercept (-1) in this fixed effects model\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c681",
   "metadata": {},
   "source": [
    "Then we run it and observe the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = sm.ols(f, data=hp2).fit()\n",
    "print(m2.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97561efb",
   "metadata": {},
   "source": [
    "The structure of the summary is a little different, but most elements should be easy to recognize. Most importantly, we see that the adjusted R-squared has risen significantly, and each of the fixed effects is a significant predictor of housing price above and beyond the original variables we included.\n",
    "\n",
    "To get a better sense of the variation in neighbourhood effects, we can actually extract them from the model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafe552",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_effects = m2.params.filter(like='NAME_2')\n",
    "neighborhood_effects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616c9a0",
   "metadata": {},
   "source": [
    "Strip off the coefficient naming convention from the table, and put them into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = neighborhood_effects.index.str.extract(r'NAME_2\\[(.*)\\]', expand=False)\n",
    "neighborhood_effects.index = stripped\n",
    "neighborhood_effects = neighborhood_effects.to_frame('fixed_effect')\n",
    "neighborhood_effects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0fc65",
   "metadata": {},
   "source": [
    "And then create a new \"NAME\" column that we can use to join back to the boroughs GeoJSON file in order to map the fixed effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_effects['NAME'] = neighborhood_effects.index\n",
    "lb2 = lb.merge(neighborhood_effects, how='left', left_on='NAME', right_on='NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(25, 20)) #Subplots allows you to draw multiple plots in one figure\n",
    "lb2.crs = (27700)\n",
    "lb2.to_crs(3857).plot(ax=ax, column='fixed_effect', legend=True, scheme='FisherJenks', k=5, edgecolor='white', alpha=.68, aspect=1)\n",
    "ax.set_axis_off() #Remove axes from plot \n",
    "ax.set_title(\"London Borough Fixed Effects\")\n",
    "ax.set_axis_off() #Remove axes from plot \n",
    "plt.axis('equal') #Set x and y axes to be equal size\n",
    "cx.add_basemap(ax, source=cx.providers.CartoDB.Voyager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ecf5a",
   "metadata": {},
   "source": [
    "This is very interesting, and we can see that there clearly is value in illustrating the heterogeneity in average house price by neighbourhood (even when controlling for the characteristics of the houses themselves). However, these fixed effects don't help us to generalize the results very much, i.e., as social scientists we are probably much more interested in finding out *what about these neighbourhoods* is making them so much more attractive, rather than just identifying the fact that some neighbourhoods are more attractive than others.\n",
    "### Spatial Feature Engineering\n",
    "One way to do this is to engineer some spatial features by directly calculating proximity to various features that we might think are structuring the data systematically. While we already calculated a few of these at the start of the practical (distance to nearest road, transit stations, and open spaces), distance to the centre of London is another valuable - if still relatively general - continuous spatial feature that captures a lot of the economic (i.e., commuting), cultural, and social benefits of being located in the \"centre of the action.\" In ```GeoPandas``` we can directly calculate the minimum distance from a spatial layer to the closest feature in another layer using the ```geometry.apply``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80477dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp_0km = hp2.geometry.progress_apply(lambda x: KM0.distance(x).min()) #KM0 = the central point of London near Trafalgar square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e399182",
   "metadata": {},
   "source": [
    "Then we join this table of distances back to our 2023 housing points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdf8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp_0km_2 = pd.merge(hp2, hp_0km, left_index=True, right_index=True)\n",
    "# hp_0km_3 = hp_0km_2.rename(index=str, columns={\"geometry_y\":\"Dist_0KM\", \"geometry_x\":\"geometry\"})\n",
    "# hp_0km_3.to_file(\"hp_0km_3.geojson\", driver='GeoJSON') # Export the data for quicker acesss in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763d300",
   "metadata": {},
   "source": [
    "However, as before, this process may take some time, so we can instead load the pre-calculated distances directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c4a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_0km_3 = gpd.read_file('../data/hp_0km_3.geojson')\n",
    "hp_0km_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8543a",
   "metadata": {},
   "source": [
    "And map them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db71c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = plt.cm.get_cmap('viridis')\n",
    "reversed_color_map = color_map.reversed()\n",
    "hp_0km_3.crs = (27700)\n",
    "f, ax = plt.subplots(1, figsize=(25, 20)) #Subplots allows you to draw multiple plots in one figure\n",
    "hp_0km_3.to_crs('EPSG:3857').plot('Dist_0KM', cmap=reversed_color_map, marker='.', s=5, ax=ax)\n",
    "ax.set_axis_off() #Remove axes from plot \n",
    "ax.set_title('Distance to Center of London in Meters') #Plot title text\n",
    "plt.axis('equal') #Set x and y axes to be equal size\n",
    "cx.add_basemap(ax, source=cx.providers.CartoDB.Voyager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6a475",
   "metadata": {},
   "source": [
    "Compare this to the maps of neighbourhood fixed effects and spatial hotspots of model residuals above - it certainly seems that distance to city centre might be an important variable structuring the pattern in housing price! Let's see how it behaves when we insert in into the original model specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = ['log_area']\n",
    "dist_names = added_variable_names + ['Dist_0KM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_0km_3 = hp_0km_3.dropna(subset=['PROFSCI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efe500",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = spreg.OLS(hp_0km_3[['log_price']].values, hp_0km_3[dist_names].values,\n",
    "                name_y='log_price', name_x=dist_names, robust='white')\n",
    "print(m3.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51781d47",
   "metadata": {},
   "source": [
    "Here it is highly-significant, and we've also been able to explain a lot more variation in $Y$ (adjusted R-squared is now .4407). Interestingly, it also appears that after controlling for the general spatial structure in house prices (i.e., distance to city centre), the 'Dist_Road' varuable has become a 'nuisance' factor (i.e., price *increases* the further away from a primary road the house is). Let's check the spatial clustering of the model residuals, since our diagnostics still indicate significant lack of normality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f603f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_residual = weights.spatial_lag.lag_spatial(w, m3.u)\n",
    "seaborn.regplot(\n",
    "    x=m3.u, \n",
    "    y=lag_residual, \n",
    "    line_kws=dict(color='orangered'),\n",
    "    ci=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe67965",
   "metadata": {},
   "source": [
    "We still see quite a positive linear relationship. Clearly there is still a strong spatial pattern to the model predictions; large residuals in one unit tend to be surrounded by large residuals in its neighbors, and vice-versa. As we have pretty much exhausted our non-spatial remedies at this point, it is likely time to employ an explicitly spatial regression specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fc5f6",
   "metadata": {},
   "source": [
    "## Spatial Regression Specifications\n",
    "### Spatial Diagnostics\n",
    "We can test the need for a spatial regression formally with several spatial regression diagnostics available in ```spreg```. If we provide the model with a spatial weights matrix ($W$), we can calculate the residual Moran's $I$ that we've been plotting here (which is an indicator of residual spatial error in the specification), as well as the robust Lagrange Multipliers for the spatial lag, spatial error, and spatial lag + error models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ec452",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4 = spreg.OLS(hp_0km_3[['log_price']].values, hp_0km_3[dist_names].values,\n",
    "                name_y='log_price', name_x=dist_names, robust='white', w=w, spat_diag=True, moran=True)\n",
    "print(m4.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774592d",
   "metadata": {},
   "source": [
    "All of our spatial diagnostic tests are highly-significant, which suggests that there is both significant spatial dependence and spatial error in the model specifications.\n",
    "\n",
    "### Spatial Lag of X Model (SLX)\n",
    "The most straightforward way to introduce spatial dependence directly into the regression specification is to calculate and include the spatial lags of the $X$ variables in the basic regression framework:\n",
    "\n",
    "$Y = X\\beta + WX\\gamma  +  \\epsilon$\n",
    "\n",
    "where $W$ is the spatial weights matrix and $\\gamma$ is the set of spatial lag coefficients for the independent variables. This model can be estimated using OLS because the spatial lags don't violate any of the basic assumptions (as other spatial models do) - they are essentially additional exogenous variables. For this reason, we could also consider this model a kind of extension of the \"spatial feature engineering\" approach. \n",
    "\n",
    "Recall that the fundamental purpose of accounting for spatial dependence in our model is to try to understand how spatial structure plays a role in determining variation in $Y$. A very common way for this to occur is through spatial spillovers from the covariates - in our example, this might mean that the price of a given house is determined not only by the size of the house itself, but also by the average size of neighbouring houses.\n",
    "\n",
    "We can implement this model by using `PySAL`'s `lag_spatial` function and renaming with an added 'w_' as a prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ac75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wx = hp_0km_3[old_names].apply(\n",
    "    lambda y: weights.spatial_lag.lag_spatial(w, y)\n",
    "# Rename the spatial lag, adding w_ to the original name\n",
    ").rename(\n",
    "    columns=lambda c: 'w_'+c)\n",
    "# Compute the spatial lag of each of those variables\n",
    "wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge original variables with the spatial lags in `wx`\n",
    "slx_exog = hp_0km_3[dist_names].join(wx)\n",
    "m5 = spreg.OLS(hp_0km_3[['log_price']].values, slx_exog.values,\n",
    "                name_y='log_price', name_x=slx_exog.columns.tolist(), robust='white', w=w, spat_diag=True, moran=True)\n",
    "print(m5.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cedc5ca",
   "metadata": {},
   "source": [
    "Let's dive into these results in a little more depth. If we go back to our equation for the SLX model, we can see that $\\beta$ describes the change in a particular unit's (let's subscript as $i$) $y$ ($y_{i}$) when $X_{ik}$ - *that unit's* - value for explanatory variable $k$ increases by one. We call this the __direct effect__. $\\gamma$, on the other hand, represents the __indirect effect__, which can be interpreted either as the change in $y_{i}$ when $X_{jk}$ increases by one (the neighbours of $i$ being designated here as $j$) or the change in $y_{j}$ when $X_{ik}$ increases by one (as [LeSage and Pace (2009, p. 37)](https://maynoothuniversity-my.sharepoint.com/:b:/g/personal/kevin_credit_mu_ie/ERorbeUoPJJIsHUlDhYVDdIBTtdC_AKU-M9O3V4WUClbJw?e=0gEKCM) show, average indirect impact *to* and *from* an observation are numerically equivalent). The interpretation depends largely on the viewpoint of the research question.\n",
    "\n",
    "You may have noticed that we've only computed the spatial lag of characteristics of the *houses themselves* - in this case, only 'log_area' - rather than the spatial features. While these lags can also be computed, interpretation of these indirect effects is more difficult - what exactly would the change in $y_{i}$ when $X_{jDist_0KM}$ increases by one mean *after controlling for* the change in $y_{i}$ when $X_{iDist_0KM}$ increases by one? Since $i$ and $j$ are, by definition, near one another, 'w_Dist_0km' and 'Dist_0km' are likely to be so collinear as to render the interpretation meaningless (or highly difficult). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6638b95",
   "metadata": {},
   "source": [
    "We can also see that the SLX specification has slightly lowered the residual spatial error in the model and both reduced the value of the Robust lag Lagrange Multiplier and increased the Adjusted-$R^2$, which suggests that inclusion of the spatial lag has further identified some of the spatial dependence in the relationships between our variables here.\n",
    "\n",
    "### Spatial Lag Model (SAR)\n",
    "The spatial lag model is used in cases where we assume there is spatial dependence or *contagion* in the *dependent* variable; in other words, cases where one unit's values are directly spilling-over to influence neighbouring units. The specification of the spatial lag model generally is:\n",
    "\n",
    "$Y = WY\\rho + X\\beta + \\epsilon$\n",
    "\n",
    "where $W$ is the spatial weights matrix and $\\rho$ is the spatial lag coefficient. In this case, we could interpret the spatial spillover process as some sort of social contagion where people set their house prices explicitly based on the prices of neighbouring prices. In terms of mathematical estimation, it is probably obvious that the $Y$ term on both sides of the equation violates the exogeneity assumption of OLS. There are various ways to estimate this equation (including Maximum Likelihood) available in ```PySAL```, but here we'll use the [General Method of Moments (GMM)](https://spatial.uchicago.edu/sites/spatial.uchicago.edu/files/12_gmm_slides.pdf) technique of Anselin (1988) which handles this endogeneity using a Two-Stage Least Squares (2SLS) approach, using the spatial lag of all explanatory variables as instruments for the endogenous lag variable.\n",
    "\n",
    "Although a bit of an aside, it is  important to note that the coefficients from spatial lag-type models cannot be directly interpreted ([LeSage and Pace, 2009, p. 34](https://maynoothuniversity-my.sharepoint.com/:b:/g/personal/kevin_credit_mu_ie/ERorbeUoPJJIsHUlDhYVDdIBTtdC_AKU-M9O3V4WUClbJw?e=0gEKCM)). Basically, when the spatial lag of $Y$ is included on the right-hand side of the equation, a change in the value of an independent variable in a given observation can potentially influence the the value of dependent variable in all other observations (i.e., a \"global spillover\") because the model takes into account other observations' dependent variable values through $WY$. From pp. 35-36: \"this impact includes the effect of feedback loops where observation $i$ affects observation $j$ and observation $j$ also affects observation $i$ as well as longer paths which might go from observation $i$ to $j$ to $k$ and back to $i$.\" In order to correctly deal with this complexity, scalar summary measures calculated by taking the partial derivatives of the expected values of $y$ with respect to the explanatory variables are used to estimate the average direct and indirect effects. Thus, in the SAR model (and its extensions, such as the spatial Durbin), the true direct and indirect effects are not directly available in the same way that they are in the SLX model. At this point, to my knowledge, `Python` does not yet contain a method for calculating these scalar summary measures for SAR models. However, for our purposes, it is useful enough to understand that the raw regression coefficients of the SAR model encompass both these direct and indirect effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393252d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m6 = spreg.GM_Lag(hp_0km_3[['log_price']].values, hp_0km_3[dist_names].values,\n",
    "                     w=w, name_y='log_price', name_x=dist_names, robust='white')\n",
    "print(m6.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92703477",
   "metadata": {},
   "source": [
    "As you can see, the spatial lag parameter is significant and positive, indicating positive spatial dependence in the dependent variable. The other coefficients remain significant in this specification; however, based on the spatial diagnostics above, we might still assume that there is some residual spatial error in the model.\n",
    "### Spatial Error Model (SEM)\n",
    "The spatial error model is used in cases where we assume there is residual spatial error in the specification of the model. This can come from a variety of sources, including: omitted spatially-structured variables, from measuring the dependent variable at a different spatial scale than the one at which it naturally occurs, or from a mismatch between the scale of predictor variables and the outcome variable. Consequently, there is often a less clear interpretation of the meaning of the spatial lag operator; it is included as a control for bias arising from spatial mis-specification. The specification of the spatial error model generally is:\n",
    "\n",
    "$Y =  X\\beta + \\lambda W \\upsilon + \\epsilon$\n",
    "\n",
    "This is essentially a partitioning of the full error term into a spatially-structured portion (with $W$ inserted into the estimation, where $\\lambda$ is the spatial error coefficient) and a remaining random portion ($\\epsilon$). Since the SEM does not involve spatial lags of the dependent variable, the estimates for parameters $\\beta$  can be interpreted in the usual regression sense (as partial derivatives). Of course, these models do not allow for indirect impacts to come from changes in the explanatory variables, similar to the case in standard OLS.\n",
    "\n",
    "The implementation in Python for this sized data takes some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc89779",
   "metadata": {},
   "outputs": [],
   "source": [
    "m7 = spreg.GM_Error_Het(hp_0km_3[['log_price']].values, hp_0km_3[dist_names].values,\n",
    "                     w=w, name_y='log_price', name_x=dist_names)\n",
    "print(m7.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a654568",
   "metadata": {},
   "source": [
    "As we can see, $\\lambda$ is not significant in this case, very small, and had relatively little effect on the other coefficients. This suggests that spatially-patterned error is not a significant conern for this model, but it may also be that the size of the data hidered the computation of the model. For this reason we will leave the SEM specification for the moment, but will return to it in the next practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01a8f8",
   "metadata": {},
   "source": [
    "### Spatial Durbin Model (SDM)\n",
    "We can also combine the SAR and SLX specifications to create the spatial 'Durbin' (SDM) model: $Y = WY\\rho + X\\beta + WX\\gamma + \\epsilon$. Now, the GMM estimation for the SAR model that we used here incorporates spatial lags of the explanatory variables in a Two-Stage Least Squares (2SLS) design. This means that a true spatial Durbin-type model is not exactly possible to create with the type of GMM model specified in `m6` (e.g.) above, because the spatial lags of $X$ are already being used in the estimation of the model. `PySAL` offers a maximum-likelihood (ML) estimation method for SAR models, `spreg.ML_Lag()`, which should be employed if estimating a true spatial Durbin. Unfortunately, the computation time for the ML SAR model in Python is currently extremely long, so we have opted for a functional workaround: calculating the spatial lag of $Y$ manually and directly inserting it into the right-hand side of the equation, to be estimated by `statsmodels`. Using statsmodels also allows us to include borough (or other) fixed effects in the model if we would like. I should also note that R has a robust spatial econometrics offerings (in the `spatialreg` and `sphet` packages, among [others](https://r-spatial.org/book/17-Econometrics.html)) that would allow you to estimate spatial Durbin (and all other spatial) models directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24796094",
   "metadata": {},
   "outputs": [],
   "source": [
    "durb_names = ['log_area'] + ['log_price']\n",
    "wx2 = hp_0km_3[durb_names].apply(\n",
    "    lambda y: weights.spatial_lag.lag_spatial(w, y)\n",
    "# Rename the spatial lag, adding w_ to the original name\n",
    ").rename(\n",
    "    columns=lambda c: 'w_'+c)\n",
    "durb = hp_0km_3.join(wx2)\n",
    "durb_names2 = dist_names + ['w_log_area'] + ['w_log_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d913f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = 'log_price ~ ' + ' + '.join(durb_names2) \n",
    "m8 = sm.ols(f2, data=durb).fit()\n",
    "print(m8.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b6925",
   "metadata": {},
   "source": [
    "As we can see, the adjusted R2 continues to improve (0.464); however, there appears to be strong multicollinearity (which is often a feature of SDM models) that may bias the coefficients, particularly between 'w_log_area' and 'w_log_price'. It seems likely that the portion of variance in house prices that neighbouring house size ('w_log_area') explains *after controlling for* neighbouring house prices is relatively small, but these results suggest that, all things being equal, your house price tends to be larger when your neighbours' houses are smaller and more expensive, which may make some sense. There is also likely still some residual spatial structure in the effects that we're modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c651ea",
   "metadata": {},
   "source": [
    "### Spatial Regimes Models\n",
    "As we have seen throughout this practical, there is significant neighbourhood-level heterogeneity evident in housing prices. We ultimately decided to use the spatial regression approach to better explain this heterogeneity in terms of general variable relationships. However, another approach that we might use is to explicitly try to explain this heterogeneity by breaking our data into \"spatial regimes\" (or sub-areas). The idea here is that we have identified significant heterogeneity by neighbourhood, and while we can control for the differences in average housing price by neighbourhood with fixed effects, we might be more interested in how the relationships between the covariates and housing price *vary* by neighbourhood.\n",
    "\n",
    "To model these spatial regimes, we essentially subset the dataset by regime and run separate regressions for each, observing changing variable relationships, significances, etc., in order to better understand how the important predictors of housing price vary by neighbourhood. With our best-fit specification for the full (pooled) model chosen (SDM), we can then subset neighbourhoods and run individual regressions for these regimes. Here are the results for Camden, near the centre of the city (North London):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81292a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "durbcam = durb[(durb[\"NAME_2\"]==\"Camden\")]\n",
    "m8cam = sm.ols(f2, data=durbcam).fit()\n",
    "print(m8cam.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd75b5e",
   "metadata": {},
   "source": [
    "And for Bromley, near the southeastern outskirts of the region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56edc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "durbb = durb[(durb[\"NAME_2\"]==\"Bromley\")]\n",
    "m8b = sm.ols(f2, data=durbb).fit()\n",
    "print(m8b.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390fb6f3",
   "metadata": {},
   "source": [
    "As you can see, there are some interesting differences in the relationships between our covariates and house prices in the two boroughs. \n",
    "\n",
    "First off, distance to transit appears to be a nuisance parameter in Camden, i.e., properties *further away* from transit stations have higher property values; it is not significant in Bromley. Alternatively, distance to nearest primary road is a house price *benefit* in Bromley - where many of the residents likely rely on cars for transportation - and insignificant in walkable, urban Camden. A similar relationship holds for distance to open space, which may reflect the general features of housing demand in each borough (e.g., people may value open space in Camen but it is not the primary feature driving valuable property). Local demographics are generally important to house prices in both boroughs, particularly for deprivation, as is general distance to the centre of London (which structures housing prices generally throughout the region). And finally, the spatial lags in Camden appear to follow the pooled model results, and are not significant in Bromley. This is interesting, as the R-squared for Bromley is also higher in general than the pooled model (.506) - perhaps this reflects the fact that, in Bromley, the specified variables explain most of the spatial variation in house prices (better than in other boroughs).\n",
    "\n",
    "Of course, more work must be done to fully interpret these results, and to understand whether more variables must be collected to better get at the relationships of interest. While these regressions are powerful tools (when properly specified and applied) for understanding the relationships between variables, they are only one piece of the overall picture of explaining and understanding spatial processes and making an argument in favour of particular relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c897903",
   "metadata": {},
   "source": [
    "## Self-Test Exercise\n",
    "1. Compare (best fit) specifications for the pooled model, Camden, Bromley, Ealing, and 5 other boroughs (of your choice); how do the road distance, transit distance, and open space distance coefficients change between these boroughs? Make a map that helps explain why you think your coefficient of interest changes between boroughs.\n",
    "2. Play around with distance values for $W$ - what effects does increasing the distance have on model outputs?\n",
    "3. Play around with different model specifications and additional variables. Create a new research question around a covariate of interest and test its significance using spatial regressions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
